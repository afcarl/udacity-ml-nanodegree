{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_height = 32\n",
    "img_width  = 32\n",
    "img_depth  = 3\n",
    "pixel_depth = 255\n",
    "\n",
    "dataset = pickle.load(open('dataset.pickle', 'rb'))\n",
    "train_dataset = dataset['train']\n",
    "test_dataset  = dataset['test']\n",
    "extra_dataset = dataset['extra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_number_edges(tops, heights, widths, lefts):\n",
    "    x1 = np.amin(lefts)\n",
    "    y1 = np.amin(tops)\n",
    "    x2 = np.amax(np.add(lefts, widths))\n",
    "    y2 = np.amax(np.add(tops, heights))\n",
    "    return (int(x1),int(y1),int(x2),int(y2))\n",
    "\n",
    "def augment_image_borders(edges, image_w, image_h):\n",
    "    x1, y1, x2, y2 = edges\n",
    "    \n",
    "    num_height = y2-y1\n",
    "    num_width = x2-x1\n",
    "    left = np.floor(x1 - .1*num_width)\n",
    "    top = np.floor(y1 - .1*num_height)\n",
    "    right = np.amin([np.ceil(x1 + 1.2*num_width), image_w])\n",
    "    bottom = np.amin([np.ceil(y1 + 1.2*num_height), image_h])\n",
    "    \n",
    "    return (left,top,right,bottom)\n",
    "\n",
    "def process_image(image, edges):\n",
    "    edges = augment_image_borders(edges, image.size[0], image.size[1])\n",
    "    image = image.crop(edges).resize((img_width,img_height), Image.ANTIALIAS).convert('L')\n",
    "    data = np.asarray(image)\n",
    "    return standarize_image_data(data)\n",
    "\n",
    "def standarize_image_data(data):\n",
    "    mean = np.mean(data, dtype='float32')\n",
    "    std = np.std(data, dtype='float32', ddof=1)\n",
    "    if std < 1e-4: std=1.\n",
    "    data = (data - mean) / std\n",
    "    return data\n",
    "\n",
    "def load_image(folder, filename):\n",
    "    return Image.open(\"%s/%s\" % (folder, filename))\n",
    "\n",
    "def create_arrays(num_samples):\n",
    "    input_dataset = np.ndarray((num_samples, img_height, img_width), dtype=np.float32)\n",
    "    label_dataset = np.ndarray((num_samples, 6), dtype=np.int32)\n",
    "    return input_dataset, label_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skip_digits = [0,6]\n",
    "def generate_input_output(dataset, images_folder):\n",
    "    \n",
    "    images = dataset['images']\n",
    "    labels = dataset['labels']\n",
    "    tops = dataset['tops']\n",
    "    heights = dataset['heights']\n",
    "    lefts = dataset['lefts']\n",
    "    widths = dataset['widths']\n",
    "\n",
    "    dataset_size = len(images)\n",
    "    inputs, outputs = create_arrays(dataset_size)\n",
    "    images_count = 0\n",
    "    \n",
    "    for i in range(dataset_size):\n",
    "        if i%5000 == 0: print(i, \"elapsed out of \", dataset_size, \"for: \", images_folder)\n",
    "\n",
    "        img = load_image(images_folder, images[i])\n",
    "        d_count = labels[i][labels[i] != 10].shape[0]\n",
    "        if d_count in skip_digits: continue\n",
    "        images_count += 1\n",
    "        edges = image_number_edges(tops[i][:d_count], heights[i][:d_count], lefts[i][:d_count], widths[i][:d_count])\n",
    "        img_array = process_image(img, edges)\n",
    "#        if i == 46:\n",
    "#            img.show()\n",
    "#            plt.figure()\n",
    "#            plt.imshow(img_array)\n",
    "#            plt.figure()\n",
    "#            img2=load_image(images_folder,images[i])\n",
    "#            plt.imshow(img2)\n",
    "#            print(lefts[i][:d_count], tops[i][:d_count], widths[i][:d_count], heights[i][:d_count])\n",
    "#            Image.fromarray(img_array).save('test.png')\n",
    "#            break\n",
    "#            print(d_count)\n",
    "        inputs[images_count-1] = img_array\n",
    "        outputs[images_count-1] = labels[i]\n",
    "    return inputs[:images_count], outputs[:images_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'elapsed out of ', 33402, 'for: ', 'train')\n",
      "(5000, 'elapsed out of ', 33402, 'for: ', 'train')\n",
      "(10000, 'elapsed out of ', 33402, 'for: ', 'train')\n",
      "(15000, 'elapsed out of ', 33402, 'for: ', 'train')\n",
      "(20000, 'elapsed out of ', 33402, 'for: ', 'train')\n",
      "(25000, 'elapsed out of ', 33402, 'for: ', 'train')\n",
      "(30000, 'elapsed out of ', 33402, 'for: ', 'train')\n",
      "(0, 'elapsed out of ', 13068, 'for: ', 'test')\n",
      "(5000, 'elapsed out of ', 13068, 'for: ', 'test')\n",
      "(10000, 'elapsed out of ', 13068, 'for: ', 'test')\n",
      "(0, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(5000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(10000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(15000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(20000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(25000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(30000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(35000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(40000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(45000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(50000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(55000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(60000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(65000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(70000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(75000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(80000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(85000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(90000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(95000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(100000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(105000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(110000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(115000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(120000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(125000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(130000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(135000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(140000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(145000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(150000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(155000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(160000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(165000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(170000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(175000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(180000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(185000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(190000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(195000, 'elapsed out of ', 202353, 'for: ', 'extra')\n",
      "(200000, 'elapsed out of ', 202353, 'for: ', 'extra')\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = generate_input_output(train_dataset, 'train')\n",
    "test_data, test_labels   = generate_input_output(test_dataset, 'test')\n",
    "extra_data, extra_labels = generate_input_output(extra_dataset, 'extra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235754, 32, 32)\n",
      "(235754, 6)\n"
     ]
    }
   ],
   "source": [
    "train_extra_data = np.append(train_data, extra_data, axis=0)\n",
    "train_extra_labels = np.append(train_labels, extra_labels, axis=0)\n",
    "\n",
    "print(train_extra_data.shape)\n",
    "print(train_extra_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_final, valid_data_final, train_labels_final, valid_labels_final = train_test_split(\n",
    "    train_extra_data, train_extra_labels, train_size=230000)\n",
    "\n",
    "test_data_final, test_labels_final = test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_pickle(filename, force=False):\n",
    "    if os.path.exists(filename + '.pickle') and not force:\n",
    "        print('%s already present - Skipping pickling.' % filename)\n",
    "    else:\n",
    "        print('Pickling %s.pickle' % filename)\n",
    "        dataset = {\n",
    "            'train': {'data':train_data_final, 'labels':train_labels_final},\n",
    "            'test':  {'data':test_data_final, 'labels':test_labels_final},\n",
    "            'valid': {'data':valid_data_final, 'labels':valid_labels_final}\n",
    "        }\n",
    "        try:\n",
    "            with open( filename + '.pickle', 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to %s.pickle' % filename, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling svhn_grayscale.pickle\n"
     ]
    }
   ],
   "source": [
    "maybe_pickle('svhn_grayscale')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
